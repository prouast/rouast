<!DOCTYPE html>
<html lang="en">
<head>
  <!-- standard meta -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
  <meta name="author" content="Philipp Rouast">
  <meta name="description" content="I am a PhD candidate at the University of Newcastle, Australia. My research focuses on human-centered applications of deep learning and computer vision.">
  <!-- site properties -->
  <title>Philipp Rouast</title>
  <!-- inject:css -->
  <link rel="stylesheet" type="text/css" href="css/styles.css">
  <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.7.5/dist/semantic.min.css">
  <!-- Global site tag - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-92777237-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-92777237-2');
  </script>
  <!-- endinject -->
</head>
<body>
  <!-- Fixed menu -->
  <div class="ui fixed inverted large menu">
    <div class="ui container">
      <a href="#top" class="item" id="topMenu">
        <i class="home icon"></i>
      </a>
      <a href="#projects" class="item" id="projectsMenu">Projects</a>
      <a href="#publications" class="item" id="publicationsMenu">Publications</a>
      <a href="#timeline" class="item" id="timelineMenu">Timeline</a>
    </div>
  </div>
  <!-- Intro -->
  <div class="ui inverted vertical center aligned segment">
    <div class="ui center aligned text container">
      <center><img class="ui small circular image" src="/img/prouast.jpg" alt="Philipp Rouast"></center>
      <h1 class="ui inverted header">
        Philipp Rouast
      </h1>
      <div class="ui relaxed inverted list">
        <div class="item">
          <a class="ui white link" href="https://www.google.com/maps/place/Newcastle+NSW+2300/">
            <i class="map marker alternate icon"></i>
            Newcastle, Australia
          </a>
        </div>
        <div class="item">
          <a class="ui white link" href="mailto:philipp@rouast.com?Subject=Oh%20hi%20philipp">
            <i class="envelope icon"></i>
            Email
          </a>
        </div>
      </div>
      <h3>I am a PhD candidate at the University of Newcastle. My research focuses on human-centered applications of deep learning and computer vision, especially in the health domain.</h3>
    </div>
  </div>
  <!-- Projects -->
  <div id="projects" class="segment">
    <div class="ui center aligned container">
      <h2 class="ui header">Projects</h2>
      <h5 class="ui horizontal divider header">
        <i class="wrench icon"></i>
        Some projects I've worked on.
      </h5>
    </div>
    <div class="ui hidden divider"></div>
    <div class="ui container">
      <div class="ui three stackable cards">
        <div class="ui card">
          <div class="image">
            <img src="/img/oreba.png" alt="OREBA dataset.">
          </div>
          <div class="content">
            <span class="header">OREBA dataset</span>
            <div class="meta">
              <i class="calendar icon"></i>
              <span class="date">2018–2020</span>
              <i class="file code icon"></i>
              <span class="code">Python</span>
            </div>
            <div class="description">
              A dataset for intake gesture detection with video and inertial data - 202 sessions with 9069 gestures.
            </div>
          </div>
          <div class="extra content">
            <a href="https://www.newcastle.edu.au/oreba" target="_blank">
              <button class="ui small button">
                <i class="download icon"></i>
                Access
              </button>
            </a>
            <a href="https://www.youtube.com/watch?v=jEoyeYqKvDg" target="_blank">
              <button class="ui small youtube button">
                <i class="youtube icon"></i>
                YouTube
              </button>
            </a>
            <a href="https://github.com/prouast/inertial-sensor-processing" target="_blank">
              <button class="ui small button">
                <i class="github icon"></i>
                GitHub
              </button>
            </a>
          </div>
        </div>
        <div class="ui card">
          <div class="image">
            <img src="/img/intake_detection.jpg" alt="Automatic detection of individual intake gestures based on 360-degree video and deep learning.">
          </div>
          <div class="content">
            <span class="header">Intake gesture detection</span>
            <div class="meta">
              <i class="calendar icon"></i>
              <span class="date">2018–2020</span>
              <i class="file code icon"></i>
              <span class="code">Python</span>
              <span class="code">TensorFlow</span>
            </div>
            <div class="description">
              Automatic detection of individual intake gestures based on 360-degree video and deep learning.
            </div>
          </div>
          <div class="extra content">
            <a href="https://github.com/prouast/deep-intake-detection" target="_blank">
              <button class="ui small button">
                <i class="github icon"></i>
                GitHub
              </button>
            </a>
            <a href="https://www.youtube.com/watch?v=O0Tj0m1yyxM" target="_blank">
              <button class="ui small youtube button">
                <i class="youtube icon"></i>
                YouTube
              </button>
            </a>
            <a>
              <button id="intakeModalButton" class="ui small green button">
                <i class="play icon"></i>
                Try
              </button>
            </a>
          </div>
        </div>
        <div class="ui card">
          <div class="image">
            <img src="/img/rppg.jpg" alt="Contactless heart rate measurement based on face video, implemented for desktop, web, and mobile">
          </div>
          <div class="content">
            <span class="header">rPPG</span>
            <div class="meta">
              <i class="calendar icon"></i>
              <span class="date">2015–2019</span>
              <i class="file code icon"></i>
              <span class="code">C++</span>
              <span class="code">JavaScript</span>
              <span class="code">OpenCV</span>
            </div>
            <div class="description">
              Contactless heart rate measurement based on face video, implemented for desktop, web, and mobile.
            </div>
          </div>
          <div class="extra content">
            <a href="https://github.com/prouast/heartbeat" target="_blank">
              <button class="ui small button">
                <i class="github icon"></i>
                GitHub
              </button>
            </a>
            <a href="https://www.youtube.com/watch?v=D_KYv7pXAvQ" target="_blank">
              <button class="ui small youtube button">
                <i class="youtube icon"></i>
                YouTube
              </button>
            </a>
            <a>
              <button id="rppgModalButton" class="ui small green button">
                <i class="play icon"></i>
                Try
              </button>
            </a>
          </div>
        </div>
      </div>
    </div>
    <div class="ui container" id="hidableProjects">
      <div class="ui three stackable cards">
        <div class="ui card">
          <div class="image">
            <img src="/img/equirectangular-remap.jpg" alt="Generate maps for conversions from spherical to equirectangular in ffmpeg.">
          </div>
          <div class="content">
            <span class="header">Equirectangular remap</span>
            <div class="meta">
              <i class="calendar icon"></i>
              <span class="date">2017</span>
              <i class="file code icon"></i>
              <span class="code">C</span>
              <span class="code">ffmpeg</span>
            </div>
            <div class="description">
              Generate maps for conversions from spherical to equirectangular in ffmpeg.
            </div>
          </div>
          <div class="extra content">
            <a href="https://github.com/prouast/equirectangular-remap" target="_blank">
              <button class="ui small button">
                <i class="github icon"></i>
                GitHub
              </button>
            </a>
          </div>
        </div>
        <div class="ui card">
          <div class="image">
            <img src="/img/cryptocurrency-analysis.jpg" alt="Correlations between returns in the cryptocurrency market.">
          </div>
          <div class="content">
            <span class="header">Cryptocurrency analysis</span>
            <div class="meta">
              <i class="calendar icon"></i>
              <span class="date">2017</span>
              <i class="file code icon"></i>
              <span class="code">R</span>
            </div>
            <div class="description">
              Analysis and visualisation of the cryptocurrency market.
            </div>
          </div>
          <div class="extra content">
            <a href="https://github.com/prouast/cryptocurrency-analysis" target="_blank">
              <button class="ui small button">
                <i class="github icon"></i>
                GitHub
              </button>
            </a>
          </div>
        </div>
        <div class="ui card">
          <div class="image">
            <img src="/img/brownie.jpg">
          </div>
          <div class="content">
            <span class="header">Brownie</span>
            <div class="meta">
              <i class="calendar icon"></i>
              <span class="date">2016</span>
              <i class="file code icon"></i>
              <span class="code">Java</span>
            </div>
            <div class="description">
              A NeuroIS tool for conducting economic experiments.
            </div>
          </div>
          <div class="extra content">
            <a href="https://im.iism.kit.edu/1093_1100.php" target="_blank">
              <button class="ui small button">
                <i class="home icon"></i>
                Project Page
              </button>
            </a>
          </div>
        </div>
      </div>
      <div class="ui three stackable cards">
        <div class="ui card">
          <div class="image">
            <img src="/img/planspiel.png">
          </div>
          <div class="content">
            <span class="header">Planspiel Flächenhandel</span>
            <div class="meta">
              <i class="calendar icon"></i>
              <span class="date">2014</span>
              <i class="file code icon"></i>
              <span class="code">JavaScript</span>
              <span class="code">Groovy</span>
              <span class="code">Grails</span>
            </div>
            <div class="description">
              Web-based simulation game for emissions certificate trading.
            </div>
          </div>
          <div class="extra content">
            <a href="http://www.flaechenhandel.de/index.html" target="_blank">
              <button class="ui small button">
                <i class="home icon"></i>
                Project Page
              </button>
            </a>
          </div>
        </div>
      </div>
    </div>
    <div class="ui hidden divider"></div>
    <div class="ui center aligned container">
      <button id="projectsButton" class="ui small basic primary button">
        Show more
      </button>
    </div>
  </div>
  <!-- Publications -->
  <div id="publications" class="segment">
    <div class="ui center aligned container">
      <h2 class="ui header">Publications</h2>
      <h5 class="ui horizontal divider header">
        <i class="pencil alternate icon"></i>
        Current and forthcoming publications.
      </h5>
    </div>
    <div class="ui hidden divider"></div>
    <div class="ui container">
      <div class="ui items">
        <div class="item">
          <div class="image">
            <img src="/img/oreba.png" alt="OREBA dataset">
          </div>
          <div class="content">
            <a href="https://arxiv.org/abs/2007.15831" class="header" target="_blank">
              OREBA: A Dataset for Objectively Recognizing Eating Behaviour and Associated Intake
            </a>
            <div class="authors">
              <span><u>Philipp V. Rouast</u>, Hamid Heydarian, Marc T. P. Adam, and Megan E. Rollo</span>
            </div>
            <div class="meta">
              <span>Preprint (2020)</span>
            </div>
            <div class="description">
              <p>
                Automatic detection of intake gestures is a key element of automatic dietary monitoring. Several types of sensors, including inertial measurement units (IMU) and video cameras, have been used for this purpose. The common machine learning approaches make use of the labelled sensor data to automatically learn how to make detections.
                <span id="dotsACC2020">...</span>
                <span id="moreACC2020">
                  One characteristic, especially for deep learning models, is the need for large datasets. To meet this need, we collected the Objectively Recognizing Eating Behavior and Associated Intake (OREBA) dataset. The OREBA dataset aims to provide a comprehensive multi-sensor recording of communal intake occasions for researchers interested in automatic detection of intake gestures. Two scenarios are included, with 100 participants for a discrete dish and 102 participants for a shared dish, totalling 9069 intake gestures. Available sensor data consists of synchronized frontal video and IMU with accelerometer and gyroscope for both hands. We report the details of data collection and annotation, as well as technical details of sensor processing. The results of studies on IMU and video data involving deep learning models are reported to provide a baseline for future research.
                </span>
                <a id="buttonACC2020">Read more</a>
              </p>
            </div>
            <div class="extra">
              <a href="pdf/rouast2020oreba.pdf" class="ui basic label" target="_blank">
                <i class="file pdf icon"></i>
                PDF
              </a>
              <span class="links"><a href="https://arxiv.org/abs/2007.15831" target="_blank">arXiv</a></span>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="image">
            <img src="/img/intake_detection_1.gif" alt="Learning deep representations for video-based intake gesture detection">
            <img src="/img/intake_detection_2.gif" alt="Learning deep representations for video-based intake gesture detection">
          </div>
          <div class="content">
            <a href="https://ieeexplore.ieee.org/document/8853283" class="header" target="_blank">
              Learning deep representations for video-based intake gesture detection
            </a>
            <div class="authors">
              <span><u>Philipp V. Rouast</u> and Marc T. P. Adam</span>
            </div>
            <div class="meta">
              <span>IEEE Journal of Biomedical and Health Informatics 24 (6), 1727–1737 (2020)</span>
            </div>
            <div class="description">
              <p>
                Automatic detection of individual intake gestures during eating occasions has the potential to improve dietary monitoring and support dietary recommendations. Existing studies typically make use of on-body solutions such as inertial and audio sensors, while video is used as ground truth. Intake gesture detection directly based on video has rarely
                <span id="dotsJBHI2019">...</span>
                <span id="moreJBHI2019">
                  been attempted. In this study, we address this gap and show that deep learning architectures can successfully be applied to the problem of video-based detection of intake gestures. For this purpose, we collect and label video data of eating occasions using 360-degree video of 102 participants. Applying state-of-the-art approaches from video action recognition, our results show that (1) the best model achieves an F1 score of 0.858, (2) appearance features contribute more than motion features, and (3) temporal context in form of multiple video frames is essential for top model performance.
                </span>
                <a id="buttonJBHI2019">Read more</a>
              </p>
            </div>
            <div class="extra">
              <a href="pdf/rouast2019learning.pdf" class="ui basic label" target="_blank">
                <i class="file pdf icon"></i>
                PDF
              </a>
              <span class="links"><a href="https://arxiv.org/abs/1909.10695" target="_blank">arXiv</a></span>
              <span class="links"><a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=9782845862844513039" target="_blank">Google Scholar</a></span>
              <span class="links"><a href="https://ieeexplore.ieee.org/document/8853283" target="_blank">IEEE Xplore</a></span>
              <span class="links"><a href="https://www.ncbi.nlm.nih.gov/pubmed/31567103" target="_blank">PubMed</a></span>
              <span class="links"><a href="https://www.researchgate.net/publication/336153618_Learning_deep_representations_for_video-based_intake_gesture_detection" target="_blank">ResearchGate</a></span>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="image">
            <img src="/img/tac.png" alt="Deep Learning for Human Affect Recognition: Insights and New Developments">
          </div>
          <div class="content">
            <a href="https://ieeexplore.ieee.org/abstract/document/8598999/" class="header" target="_blank">
              Deep Learning for Human Affect Recognition: Insights and New Developments
            </a>
            <div class="authors">
              <span><u>Philipp V. Rouast</u>, Marc T. P. Adam, Raymond Chiong</span>
            </div>
            <div class="meta">
              <span>IEEE Transactions on Affective Computing (2019)</span>
            </div>
            <div class="description">
              <p>
                Automatic human affect recognition is a key step towards more natural human-computer interaction. Recent trends include recognition in the wild using a fusion of audiovisual and physiological sensors, a challenging setting for conventional machine learning algorithms. Since 2010, novel deep learning algorithms have been applied increasingly in this field.
                <span id="dotsTAC2019">...</span>
                <span id="moreTAC2019">
                  In this paper, we review the literature on human affect recognition between 2010 and 2017, with a special focus on approaches using deep neural networks. By classifying a total of 950 studies according to their usage of shallow or deep architectures, we are able to show a trend towards deep learning. Reviewing a subset of 233 studies that employ deep neural networks, we comprehensively quantify their applications in this field. We find that deep learning is used for learning of (i) spatial feature representations, (ii) temporal feature representations, and (iii) joint feature representations for multimodal sensor data. Exemplary state-of-the-art architectures illustrate the recent progress. Our findings show the role deep architectures will play in human affect recognition, and can serve as a reference point for researchers working on related applications.
                </span>
                <a id="buttonTAC2019">Read more</a>
              </p>
            </div>
            <div class="extra">
              <a href="pdf/rouast2019deep.pdf" class="ui basic label" target="_blank">
                <i class="file pdf icon"></i>
                PDF
              </a>
              <span class="links"><a href="https://arxiv.org/abs/1901.02884" target="_blank">arXiv</a></span>
              <span class="links"><a href="https://scholar.google.com.au/scholar?cluster=17643367748425293417" target="_blank">Google Scholar</a></span>
              <span class="links"><a href="https://ieeexplore.ieee.org/abstract/document/8598999/" target="_blank">IEEE Xplore</a></span>
              <span class="links"><a href="https://www.researchgate.net/publication/329980556_Deep_Learning_for_Human_Affect_Recognition_Insights_and_New_Developments" target="_blank">ResearchGate</a></span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="ui container" id="hidablePublications">
      <div class="ui items">
        <div class="item">
          <div class="image">
            <img src="/img/fcs.png" alt="Remote heart rate measurement using low-cost RGB face video: a technical literature review">
          </div>
          <div class="content">
            <a href="https://link.springer.com/article/10.1007/s11704-016-6243-6" class="header" target="_blank">
              Remote heart rate measurement using low-cost RGB face video: a technical literature review
            </a>
            <div class="authors">
              <span><u>Philipp V. Rouast</u>, Marc T. P. Adam, Raymond Chiong, David Cornforth, Eva Lux</span>
            </div>
            <div class="meta">
              <span>Frontiers of Computer Science 12 (5), 858–872</span>
            </div>
            <div class="description">
              <p>
                Remote photoplethysmography (rPPG) allows remote measurement of the heart rate using low-cost RGB imaging equipment. In this study, we review the development of the field of rPPG since its emergence in 2008. We also classify existing rPPG approaches and derive a framework that provides an overview of modular steps.
                <span id="dotsFCS2016">...</span>
                <span id="moreFCS2016">
                  Based on this framework, practitioners can use our classification to design algorithms for an rPPG approach that suits their specific needs. Researchers can use the reviewed and classified algorithms as a starting point to improve particular features of an rPPG algorithm.
                </span>
                <a id="buttonFCS2016">Read more</a>
              </p>
            </div>
            <div class="extra">
              <a href="pdf/rouast2016remote_a.pdf" class="ui basic label" target="_blank">
                <i class="file pdf icon"></i>
                PDF
              </a>
              <span class="links"><a href="https://scholar.google.com.au/scholar?cluster=7619880296834415536" target="_blank">Google Scholar</a></span>
              <span class="links"><a href="https://www.researchgate.net/publication/306285292_Remote_heart_rate_measurement_using_low-cost_RGB_face_video_A_technical_literature_review" target="_blank">ResearchGate</a></span>
              <span class="links"><a href="https://link.springer.com/article/10.1007/s11704-016-6243-6" target="_blank">SpringerLink</a></span>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="image">
            <img src="/img/ecis.png" alt="Using deep learning and 360 video to detect eating behavior for user assistance systems">
          </div>
          <div class="content">
            <a href="https://aisel.aisnet.org/ecis2018_rp/101/" class="header" target="_blank">
              Using deep learning and 360 video to detect eating behavior for user assistance systems
            </a>
            <div class="authors">
              <span><u>Philipp V. Rouast</u>, Marc T. P. Adam, Tracy Burrows, Raymond Chiong, Megan Rollo</span>
            </div>
            <div class="meta">
              <span>European Conference on Information Systems (ECIS 2018)</span>
            </div>
            <div class="description">
              <p>
                The rising prevalence of non-communicable diseases calls for more sophisticated approaches to support individuals in engaging in healthy lifestyle behaviors, particularly in terms of their dietary intake. Building on recent advances in information technology, user assistance systems hold the potential of combining active and passive data collection methods to
                <span id="dotsECIS2018">...</span>
                <span id="moreECIS2018">
                  monitor dietary intake and, subsequently, to support individuals in making better decisions about their diet. In this paper, we review the state-of-the-art in active and passive dietary monitoring along with the issues being faced. Building on this groundwork, we propose a research framework for user assistance systems that combine active and passive methods with three distinct levels of assistance. Finally, we outline a proof-of-concept study using video obtained from a 360-degree camera to automatically detect eating behavior from video data as a source of passive dietary monitoring for decision support.
                </span>
                <a id="buttonECIS2018">Read more</a>
              </p>
            </div>
            <div class="extra">
              <a href="pdf/rouast2018using.pdf" class="ui basic label" target="_blank">
                <i class="file pdf icon"></i>
                PDF
              </a>
              <span class="links"><a href="https://aisel.aisnet.org/ecis2018_rp/101/" target="_blank">AIS eLibrary</a></span>
              <span class="links"><a href="https://scholar.google.com.au/scholar?cluster=15963458291069625288" target="_blank">Google Scholar</a></span>
              <span class="links"><a href="https://www.researchgate.net/publication/328475515_Using_Deep_Learning_and_360_Video_to_Detect_Eating_Behavior_for_User_Assistance_Systems" target="_blank">ResearchGate</a></span>
            </div>
          </div>
        </div>
        <div class="item">
          <div class="image">
            <img src="/img/aitic.png" alt="Remote photoplethysmography: Evaluation of contactless heart rate measurement in an information systems setting">
          </div>
          <div class="content">
            <a class="header">Remote photoplethysmography: Evaluation of contactless heart rate measurement in an information systems setting</a>
            <div class="authors">
              <span><u>Philipp V. Rouast</u>, Marc T. P. Adam, Verena Dorner, Eva Lux</span>
            </div>
            <div class="meta">
              <span>Applied Informatics and Technology Innovation Conference (AITIC 2016)</span>
            </div>
            <div class="description">
              <p>
                As a source of valuable information about a person’s affective state, heart rate data has the potential to improve both understanding and experience of human-computer interaction. Conventional methods for measuring heart rate use skin contact methods, where a measuring device must be worn by the user. In an Information
                <span id="dotsAITIC2016">...</span>
                <span id="moreAITIC2016">
                  Systems setting, a contactless approach without interference in the user’s natural environment could prove to be advantageous. We develop an application that fulfils these conditions. The algorithm is based on remote photoplethysmography, taking advantage of the slight skin color variation that occurs periodically with the user’s pulse. When evaluating this application in an Information Systems setting with various arousal levels and naturally moving subjects, we achieve an average root mean square error of 7.32 bpm for the best performing configuration. We find that a higher frame rate yields better results than a larger size moving measurement window. Regarding algorithm specifics, we find that a more detailed algorithm using the three RGB signals slightly outperforms a simple algorithm using only the green signal.
                </span>
                <a id="buttonAITIC2016">Read more</a>
              </p>
            </div>
            <div class="extra">
              <a href="pdf/rouast2016remote_b.pdf" class="ui basic label" target="_blank">
                <i class="file pdf icon"></i>
                PDF
              </a>
              <span class="links"><a href="https://scholar.google.com.au/scholar?oi=bibs&hl=en&cluster=16146844465437811687" target="_blank">Google Scholar</a></span>
              <span class="links"><a href="https://www.researchgate.net/publication/329228553_Remote_Photoplethysmography_Evaluation_of_Contactless_Heart_Rate_Measurement_in_an_Information_Systems_Setting" target="_blank">ResearchGate</a></span>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="ui hidden divider"></div>
    <div class="ui center aligned container">
      <button id="publicationsButton" class="ui small basic primary button">
        Show more
      </button>
    </div>
  </div>
  <!-- Timeline -->
  <div id="timeline" class="segment">
    <div class="ui center aligned container">
      <h2 class="ui header">Timeline</h2>
      <h5 class="ui horizontal divider header">
        <i class="graduation cap icon"></i>
        Timeline of my positions and education.
      </h5>
    </div>
    <div class="ui hidden divider"></div>
    <div class="ui container">
      <div class="ui stackable divided two column grid">
        <div class="column">
          <center><h3>Positions</h3></center>
          <div class="ui large feed">
            <div class="event">
              <div class="label">
                <img src="/img/uon.png">
              </div>
              <div class="content">
                <div class="summary">
                  <a href="https://www.newcastle.edu.au/" target="_blank">The University of Newcastle</a>: Teaching Assistant (<a href="https://www.newcastle.edu.au/about-uon/governance-and-leadership/faculties-and-schools/faculty-of-engineering-and-built-environment/school-of-electrical-engineering-and-computing" target="_blank">SEEC</a>)
                  <div class="date">
                    2015; 2017–now
                  </div>
                </div>
                <div class="extra text">
                  <ul class="ui list">
                    <li>EBUS3050: The Digital Economy</li>
                    <li>INFT2150: Business Analysis</li>
                    <li>INFT6201: Big Data</li>
                    <li>COMP1010: Computing Fundamentals</li>
                  </ul>
                </div>
              </div>
            </div>
            <div class="event">
              <div class="label">
                <img src="/img/kit.png">
              </div>
              <div class="content">
                <div class="summary">
                  <a href="https://www.kit.edu/english/index.php" target="_blank">Karlsruhe Institute of Technology</a>: Student Research Assistant (<a href="https://im.iism.kit.edu/english/index.php" target="_blank">IISM</a>)
                  <div class="date">
                    2012–2015; 2016
                  </div>
                </div>
                <div class="extra text">
                  <ul class="ui list">
                    <li>Development of a web-based prediction market (Groovy/Grails)</li>
                    <li>Development of experiment platform <a href="https://im.iism.kit.edu/1093_1100.php" target="_blank">Brownie</a> (Java)</li>
                  </ul>
                </div>
              </div>
            </div>
            <div class="event">
              <div class="label">
                <img src="/img/msg.jpg">
              </div>
              <div class="content">
                <div class="summary">
                  <a href="https://www.msg-gillardon.de/" target="_blank">msgGillardon AG</a>: Internship
                  <div class="date">
                    2013–2014
                  </div>
                </div>
                <div class="extra text">
                  <ul class="ui list">
                    <li>Evaluation of assumptions in the Credit Risk Model <i>CreditMetrics</i></li>
                    <li>Implementing improvements for Loss Given Default estimation for retail credits</li>
                  </ul>
                </div>
              </div>
            </div>
            <div class="event">
              <div class="label">
                <img src="/img/kit.png">
              </div>
              <div class="content">
                <div class="summary">
                  <a href="https://www.kit.edu/english/index.php" target="_blank">Karlsruhe Institute of Technology</a>: Teaching Assistant (<a href="http://aifb.kit.edu/web/Hauptseite/en" target="_blank">AIFB</a>)
                  <div class="date">
                    2011–2012
                  </div>
                </div>
                <div class="extra text">
                  <ul class="ui list">
                    <li>Programming I: Java</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="column">
          <center><h3>Education</h3></center>
          <div class="ui large feed">
            <div class="event">
              <div class="label">
                <img src="/img/uon.png">
              </div>
              <div class="content">
                <div class="summary">
                  <a href="https://www.newcastle.edu.au/" target="_blank">The University of Newcastle</a>: PhD
                  <div class="date">
                    2017–now
                  </div>
                </div>
                <div class="ui small label">
                  <i class="yellow star icon"></i> Int. Postgraduate Research Scholarship
                </div>
                <div class="ui small label">
                  <i class="yellow trophy icon"></i> <a href="https://www.acphis.org.au" target="_blank">ACPHIS</a> Student Project Award 2017
                </div>
                <div class="ui small label">
                  <i class="yellow trophy icon"></i> 2017 UON FEBE Postgraduate Research Prize
                </div>
                <div class="ui small label">
                  <i class="yellow trophy icon"></i> 2019 UON FEBE Postgraduate Research Prize
                </div>
                <div class="extra text">
                  <b>Thesis</b>: Using deep learning to detect food intake behaviour from video.
                </div>
              </div>
            </div>
            <div class="event">
              <div class="label">
                <img src="/img/kit.png">
              </div>
              <div class="content">
                <div class="summary">
                  <a href="https://www.kit.edu/english/index.php" target="_blank">Karlsruhe Institute of Technology</a>: MSc
                  <div class="date">
                    2014–2016
                  </div>
                </div>
                <div class="ui small label">
                  <i class="yellow trophy icon"></i> Future Award 2016: Category Health
                </div>
                <div class="ui small label">
                  <i class="yellow star icon"></i> DAAD FIT Worldwide Scholarship
                </div>
                <div class="ui small label">
                  <i class="yellow star icon"></i> BW Study Abroad Scholarship
                </div>
                <div class="extra text">
                  <b>Thesis</b>: Contactless Heart Rate Measurement Using Facial Video: A Real-Time Approach and Evaluation in Information Systems.
                </div>
              </div>
            </div>
            <div class="event">
              <div class="label">
                <img src="/img/kit.png">
              </div>
              <div class="content">
                <div class="summary">
                  <a href="https://www.kit.edu/english/index.php" target="_blank">Karlsruhe Institute of Technology</a>: BSc
                  <div class="date">
                    2010–2013
                  </div>
                </div>
                <div class="extra text">
                  <b>Thesis</b>: Partisan Trading Activity: Investigation of a Political Stock Market.
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
  <!-- Footer -->
  <div class="ui inverted vertical footer segment">
    <div class="ui container">
      <div class="ui six column center aligned grid">
        <div class="one wide column">
          <a class="ui white link" href="https://github.com/prouast" target="_blank">
            <i class="github large icon"></i>
          </a>
        </div>
        <div class="one wide column">
          <a class="ui white link" href="https://scholar.google.com.au/citations?user=Ny9vtjcAAAAJ&hl=en" target="_blank">
            <i class="google large icon"></i>
          </a>
        </div>
        <div class="one wide column">
          <a class="ui white link" href="https://www.instagram.com/prouast/" target="_blank">
            <i class="instagram large icon"></i>
          </a>
        </div>
        <div class="one wide column">
          <a class="ui white link" href="https://www.linkedin.com/in/prouast" target="_blank">
            <i class="linkedin large icon"></i>
          </a>
        </div>
        <div class="one wide column">
          <a class="ui white link" href="https://stackoverflow.com/users/3595278/prouast" target="_blank">
            <i class="stack overflow large icon"></i>
          </a>
        </div>
        <div class="one wide column">
          <a class="ui white link" href="https://www.twitter.com/prouast" target="_blank">
            <i class="twitter large icon"></i>
          </a>
        </div>
      </div>
    </div>
  </div>
  <!-- Intake demo -->
  <div class="ui small modal" id="intakeModal">
    <div class="ui inverted active dimmer" id="intakeModalDimmer">
      <div class="ui text loader">Loading tensorflow...</div>
    </div>
    <div class="header">
      Intake gesture detection (alpha version <i class="flask icon"></i>)
    </div>
    <div class="content">
      <p>In this demo, the small version of our Small 3D CNN for intake gesture recognition runs directly in your browser. It takes one raw video frame at a time as input to predict the <font color="red">frame-level probability of an intake event</font>. These probabilities are displayed in the graph on the right.</p>
      <p>For best results, place device on a table and sit such that the upper body fills most of the video.</p>
      <div class="ui stackable two column grid">
        <div class="six wide column">
          <div class="webcam-box-outer">
            <div class="ui inverted dimmer" id="intakeCameraLoader">
              <div class="ui text loader">Waiting for camera...</div>
            </div>
            <div class="webcam-box-inner">
              <video autoplay playsinline muted id="intakeWebcam" width="192" height="192"></video>
            </div>
          </div>
        </div>
        <div class="ten wide column">
          <div class="ui text loader" id="intakeGraphLoader">Waiting for model</div>
          <canvas id="intakeChart"></canvas>
        </div>
      </div>
    </div>
  </div>
  <!-- rPPG demo -->
  <div class="ui small modal" id="rppgModal">
    <div class="ui inverted active dimmer" id="rppgModalDimmer">
      <div class="ui text loader">Waiting to load opencv...</div>
    </div>
    <div class="header">
      Heartrate detection (alpha version <i class="flask icon"></i>)
    </div>
    <div class="content">
      <p>This demo runs a simple variant of rPPG directly in your browser to measure your heart rate based on subtle changes in skin color.</p>
      <p>For best results, try in a constantly well lit space with minimal device and subject motion.</p>
      <video hidden autoplay playsinline muted id="rppgWebcam" width="480" height="320"></video>
      <div class="ui segment">
        <canvas id="rppgCanvas" width="480" height="320"></canvas>
        <div class="ui inverted dimmer" id="rppgCameraLoader">
          <div class="ui text loader">Waiting for camera...</div>
        </div>
      </div>
    </div>
  </div>
  <!-- JS -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fomantic-ui@2.7.5/dist/semantic.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@2.8.0"></script>
  <script type="module" src="js/index.js"></script>
  <!-- /JS -->
</body>
</html>
